# Uber ETL Data Pipeline and Analytics Dashboard | Python | Google Cloud Storage | Mage | ETL | Big Query | Looker


![image](https://private-user-images.githubusercontent.com/81063457/257497004-2f929820-68b1-4b3c-9d66-87bdc3f757bf.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQxNzQxNDcsIm5iZiI6MTcwNDE3Mzg0NywicGF0aCI6Ii84MTA2MzQ1Ny8yNTc0OTcwMDQtMmY5Mjk4MjAtNjhiMS00YjNjLTlkNjYtODdiZGMzZjc1N2JmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAxMDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMTAyVDA1MzcyN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJhNjM0OTcxMWNiMTFkZDI1ODI5MmQ2Y2E0Y2FlYzlhOWVkMWNhY2FmZjEwZjE2YjY4ZTljOGIxOTQ1YTM3NTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.tLdLvuHNN4BvGi3TTXU1KivX43luuQWlaMNYjqomL0Q
)



## Problem Statment
Perform data analtyics on Uber Cab data and create dashboard with insights from the given dataset.

## Technology Stack
Programming Language - Python
Google Cloud Platform
Google Storage
Compute Instance
BigQuery
Looker Studio
Mage ETL Data Pipeine Tool - https://www.mage.ai/


## Step 1 : Architecture Diagram



![image](https://github.com/pranav98711/UberDataPipelineAndAnalyticsDashboard/assets/58882791/b8a9c1f1-dd7d-46b9-917e-edf62e48b6e2)

## Step 2: Data Modeling and creating an ER Diagram to get a better understanding of the data

## Step 3: Writing the Transformation code in Python

## Step 4: Create a project and a bucket on the Google Cloud Platform, upload the data, select the server and set the appropriate permissions.

## Step 5: Create a Virtual Machine Instance in GCP using GCP Compute Engine.

![image](https://private-user-images.githubusercontent.com/81063457/257501434-83e73e1f-8956-4caf-8a9b-904e506fada1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQxNzQxNDcsIm5iZiI6MTcwNDE3Mzg0NywicGF0aCI6Ii84MTA2MzQ1Ny8yNTc1MDE0MzQtODNlNzNlMWYtODk1Ni00Y2FmLThhOWItOTA0ZTUwNmZhZGExLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAxMDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMTAyVDA1MzcyN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA3ZmUyNzc4OTgzZTQwZWQxNWQzMGRjODg2ZWZiOTY5ZWNmNDUwYjI3NmM5MGE2MDQ0MzQ2OTc5ODgxOWNkYjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.3Tjram_q5VOXHJhMgSv2FLYcFHApjN8rUj3YAwzfpqY)

## Step 6: Connect the VM to Mage Project using SSH Linux Terminal and create a mage project (also download the necessary dependencies).

## Step 7: Create a data pipeline using Mage Blocks like data loader, transformer, and exporters. Add your transformation code to the data transformer with the necessary changes.

## Step 8: Once, the pipeline is ready, add GCP credentials credentials to the configuration 'io_config.yaml' file. You can easily get the credentials from the APIs and Services tab from Google Console.

## Step 9: Using BigQuery to query the data, perform ETL operations so that data can be used for Data Analysis like creating dashboards, reporting, etc.

## Step 10: Finally, create a dashboard using any dashboarding/reporting software, I used Looker Studio but we can also use other tools like Power BI, Tableau, Qlik, etc.

## View Live Dashboard Here: 
[https://lookerstudio.google.com/s/nQI06ax2wMY](https://lookerstudio.google.com/s/pvJN8GhzsYQ)https://lookerstudio.google.com/s/pvJN8GhzsYQ







